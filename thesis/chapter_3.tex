
\label{chapter3}
In this chapter, we present an extension to the knockoffs procedure described in \Cref{chapter2}. The idea is to construct a family of knockoffs rather than a singlet set of knockoffs, and we will present the necessary generalizations and formalizations to do so. As far as the writer is aware, the concept of multiple knockoffs has been suggested in \cite{knockoffs} as well as \cite{panning}, but not formalized rigorously up to this point.
\section{Definition}
The definition of multiple knockoffs variables is a direct extension of the work done in \cite{knockoffs} and \cite{panning} that we revisited in \Cref{chapter2}. We define multiple model-X knockoffs as follows.
\begin{defn}
Multiple model-X knockoffs for a family of random variables $X = (X_1, \ldots, X_p)$ are a collection of $m$ families of new random variables $\tilde{X}^{(i)} = (\tilde{X}^{(i)}_1, \ldots \tilde{X}^{(i)}_p)$ constructed satisfying the following two properties, analogous to those of knockoffs: (1) \emph{Exchangeability.} Let $[[m]]$ denote the set $\{ 0, \ldots, m \}$. For each $1 \leq j \leq p$, let $\sigma_j : [[m]] \to [[m]]$ be any permutation of the indices $0$ to $m$. Then, letting $\tilde{X}^{(0)} = X$,
\begin{equation}
(\tilde{X}^{(0)}, \ldots, \tilde{X}^{(m)}) \overset{d}= (\tilde{X}^{(\sigma_1(0))}_{1}, \ldots, \tilde{X}^{(\sigma_p(0))}_{p}, \ldots, \tilde{X}^{(\sigma_1(m))}_{1}, \ldots, \tilde{X}^{(\sigma_p(m))}_{p}  )
\end{equation}
For convenience, we write
\begin{equation}
\{ \sigma_i \}_{i=1}^p \left( \tilde{X}^{(0)}, \ldots, \tilde{X}^{(m)} \right) \overset{\on{def}}{=} (\tilde{X}^{(\sigma_1(0))}_{1}, \ldots, \tilde{X}^{(\sigma_p(0))}_{p}, \ldots, \tilde{X}^{(\sigma_1(m))}_{1}, \ldots, \tilde{X}^{(\sigma_p(m))}_{p}  ).
\end{equation}
(2) \emph{Conditional Independence.} $(\tilde{X}^{(1)}, \ldots, \tilde{X}^{(m)}) \indep Y \mid X$ if there is a response variable $Y$. This property is guaranteed if the $\tilde{X}^{(i)}$ are constructed without looking at $Y$.
\end{defn}
Here, the two conditions are analogous to the ones presented in Definition 3.1 of \cite{panning}, which was restated in \Cref{knockdef}. In fact, the second condition is identical, while the first only seeks to generalize the idea of pairwise exchangeability.

\section{Exchangeability of Null Covariates and Knockoffs}
First, we provide a generalization of Lemma 3.2 in \cite{panning}. In particular, we extend the proof that we can permute null covariates with their knockoffs without changing the joint distribution of $X$ and the knockoffs $\tilde{X}^{(i)}$, conditional on $Y$.
\begin{lem}
\label{lemma2.3}
Let $S \subseteq \mathcal{H}_0$ be a subset of nulls. Consider a set of permutations $\sigma_j : [[ m ]] \to [[ m ]]$ such that if $j \not \in S$, then $\sigma_j = \on{id}_{[[ m ]]}$. Then,
\[(\tilde{X}^{(0)}, \ldots, \tilde{X}^{(m)}) \mid Y \overset{d}= \{ \sigma_i \}_{i=1}^p \left( \tilde{X}^{(0)}, \ldots, \tilde{X}^{(m)} \right) \mid Y. \]
\end{lem}
\begin{proof}
The proof is quite similar to that of the original lemma. Without loss of generality, we can assume that $S = \{ 1, \ldots, M \}$. Then, since the marginal distribution of $Y$ is the same on both sides of the equation, it is equivalent to show that the joint distributions are the same. Then, in the same way as the original lemma, by the exchangeability condition that \[ (\tilde{X}^{(0)}, \ldots, \tilde{X}^{(m)}) \overset{d}= \{ \sigma_i \}_{i=1}^p \left( (\tilde{X}^{(0)}, \ldots, \tilde{X}^{(m)}) \right),\] so the only thing we need to show is that 
\begin{equation}
Y \mid (\tilde{X}^{(0)}, \ldots, \tilde{X}^{(m)}) \overset{d}= Y \mid \{ \sigma_i \}_{i=1}^p \left( (\tilde{X}^{(0)}, \ldots, \tilde{X}^{(m)}) \right).
\end{equation}
To see this, let $p_{Y \mid X}(y|x)$ be the conditional distribution of $Y$ given $X$. Then, note that
\begin{align*}
p_{Y \mid \{ \sigma_i \}_{i=1}^p \left( \tilde{X}^{(0)}, \ldots, \tilde{X}^{(m)} \right)}(y | (\tilde{x}^{(0)}, \ldots , \tilde{x}^{(m)})) &= p_{Y \mid (\tilde{X}^{(0)}, \ldots, \tilde{X}^{(m)})} (y| \{ \sigma_i\inv \}_{i=1}^p (\tilde{x}^{(0)}, \ldots, \tilde{x}^{(m)}) ) \\
&= p_{Y \mid \tilde{X}^{(0)}} (y| x'),
\end{align*}
where $x_i' = \tilde{x}^{(\sigma_i\inv(0))}_i$ if $i \in S$ and $x_i' = x_i$ otherwise. In particular, the second equality above comes from the fact that $Y$ is conditionally independent of the knockoffs $(\tilde{X}^{(1)}, \ldots, \tilde{X}^{(m)})$ given $X$ by definition of multiple knockoffs.

Next, note that we assumed earlier that $S = \{ 1, \ldots, M \}$ is the subset of nulls. Then, by definition, $Y$ and $X_1$ will be conditionally independent given $X_{2 : p}$, we may further simplify that
\begin{align*}
p_{Y \mid \tilde{X}^{(0)}} (y| x') &= p_{Y \mid \tilde{X}^{(0)}_{1 : p}} (y | \tilde{x}^{(\sigma_1\inv(0))}_1, x'_{2 : p}) = p_{Y \mid \tilde{X}^{(0)}_{2 : p}} (y | x'_{2 : p}) = p_{Y \mid \tilde{X}^{(0)}_{1 : p}} (y | x_1^{(0)}, x'_{2 : p})
\end{align*}
This shows that the conditional distributions of $Y$ are the same:
\[ Y \mid \{ \sigma_i \}_{i=1}^p \left( (\tilde{X}^{(0)}, \ldots, \tilde{X}^{(m)}) \right) = Y \mid \{ \sigma_i' \}_{i=1}^p \left( (\tilde{X}^{(0)}, \ldots, \tilde{X}^{(m)}) \right), \] where $\sigma_1' = \on{id}_{[[m]]}$ and $\sigma_i' = \sigma_i$ for $i  > 1$. This reduces $S$ to the case where $1 \not \in S$, so we may repeat this argument until $S$ is empty, and this proves the claim.
\end{proof}

\section{Feature Statistics and \texorpdfstring{$p$}{p}-values}
In the original work on knockoffs, statistics denoted $W_j$ are computed for each of the $X_j$ based on both $X_j$ and its knockoff, where a large positive value of $W_j$ indicates evidence for significance of the covariate. Furthermore, $W_j$ is required to satisfy the flip-sign property for null $X_j$, which says that swapping $X_j$ with its knockoff has the effect of reversing the sign of $W_j$. These statistics are then used in a sequential process when running the knockoffs procedure.

Returning to multiple knockoffs, we now wish to generalize the concept of the statistic $W_j$. To be informal, there are two properties of $W_j$ that are critical to the knockoffs procedure: the sign of $W_j$, which determines the $p$-value that is used in the selection process, and the magnitude of $W_j$, which determines the ordering of the $W_j$ in the selection process. As such, to generalize to multiple knockoffs, we just need to ensure that we have generalizations of the ideas of obtaining a $p$-value from our statistic and obtaining a magnitude of our statistic. 

To be more precise, we may refer to the original knockoffs paper \cite{knockoffs} and view the knockoffs procedure as a sequential selection procedure. A key idea in the proof of the main theorems, Theorem 1 and Theorem 2, in \cite{knockoffs} is the conversion of the sign of $W_j$ to $1$-bit $p$-values: covariates such that $W_j > 0$ are assigned a $p$-value of $0.5$, whereas the covariates such that $W_j < 0$ are given a $p$-value of $1$. This conversion allows us to prove the desired statements via Theorem 3 of \cite{knockoffs}, which provides proof of control of FDR for generalized sequential testing procedures, the FSTP and SSTP. Note that generating $W_j$ statistics is in correspondence with generating statistics $Z_j$ and $\tilde{Z_j}$ for the covariate and its knockoff respectively via the formula
\begin{equation}
W_j = f_j(Z_j, \tilde{Z_j})
\end{equation}
where $f_j$ is an anti-symmetric function. 

In the case of multiple knockoffs, instead of thinking about $W_j$, we will consider $Z_j^{(i)}$ for each knockoff $i$, retaining the convention that the case of $i = 0$ is the statistic corresponding to the original covariate. As such, we can generalize the definition of the ordering statistics to become
\begin{equation}
(Z^{(0)},\ldots, Z^{(m)}) = t([\tilde{X}^{(0)}, \ldots, \tilde{X}^{(m)}], y)
\end{equation}
Here, the $Z^{(k)}_j$ are intended to measure the importance of the covariate (or knockoff) $\tilde{X}^{(k)}_j$. 

Now, we must consider the other critical component to the knockoff procedure: the magnitude of $W_j$, which determines the order in which we look at the covariates in the sequential selection procedure. Here, similar to the choice of anti-symmetric function we make for $f_j(Z_j, \tilde{Z}_j) = W_j$ in the original knockoffs case, a choice must be made to determine the ``magnitude'' of the statistic, denoted $M_j$, which can also write as $g_j(Z^{(0)}_j,\ldots, Z^{(m)}_j)$. In addition, noting that $f_j$ is anti-symmetric in the original case, we will require that $g_j$ is a symmetric function in its arguments (the analog here is that the magnitude of $f_j$ would be symmetric). 


Now, we present a lemma which is the desired generalization of Lemma 3.3 in \cite{panning}.
\begin{lem}
\label{lemma2.4}
For the null $X_j$, where $j \in \mathcal{H}_0$, let $A_{j, (i)}$ denote the $i$-th order statistic of $Z_j^{(0)}, \ldots, Z_j^{(n)}$. Then, conditional on $(M_1, \ldots, M_p)$,
\begin{equation}
\mathbb{P}\left(Z_j^{(0)} = A_{j, (i)} \right) = \frac{1}{m+1}, \qquad 1 \leq i \leq m + 1.
\end{equation}
Furthermore, the distribution of the ordering of the $Z_j^{(i)}$ will be uniform over all permutations.
\end{lem}
\begin{proof}
Like Lemma 3.3 in \cite{panning}, this is a direct consequence of \hyperref[lemma2.3]{\Cref{lemma2.3}}, and we will provide an analogous proof. From the lemma, we may deduce that $T \overset{d}{=} \{ \sigma_i \}_{i=1}^p \left( T \right)$ where the $\sigma_i$ satisfy the condition of the lemma (ie. are non-trivial only for null $j$). Then, by symmetry, the statistic $Z_j^{(0)}$ must be equally likely to be $i$-th largest statistic among the $Z_j^{(i)}$, as desired.
\end{proof}
The above lemma allows us to obtain the analogous version of $p$-values for multiple knockoffs. Using the same notation as in \hyperref[lemma2.4]{\Cref{lemma2.4}} for the covariate $X_j$, we define the $p$-value
\begin{equation}
p_j = 1 - \frac{i - 1}{m+1}, \qquad Z_j^{(0)} = A_{j, (i)}.
\end{equation}
Note that the special case where $n = 1$ simplifies directly into the original knockoffs framework.

\section{The Procedure}
Now that we have developed the analog of the desired test statistic in the setting of multiple knockoffs, we may briefly describe the procedure itself. The parallel here is rather direct, and is simply using the machinery that we have developed in the framework of \cite{knockoffs} under the FSTP and SSTP described there. We will present these results in a similar fashion to \cite{panning}.
\begin{theorem}
Fix a threshold $c \in (0,1)$. Then, choose a threshold $\tau > 0$ by setting
\[ \tau = \min \left\{ t > 0 : \frac{\# \{ j : M_j \geq t, p_j > c \}}{\# \{ j : M_j \geq t, p_j \leq c \}} \leq q \right\} \qquad \textbf{(MultipleKnockoffs)} \] where $q$ is the target FDR level (or $\tau = + \infty$ if the set above is empty). Then, the procedure that selects the variables
\[ \hat{S} = \{ j : M_j \geq t, p_j \leq c \} \] controls the modified FDR defined as
\[ \on{mFDR} = \BE\left[ \frac{|\{ j \in \hat{S} \cap \mathcal{H}_0 \}|}{|\hat{S}|+ 1 / q} \right] \leq q. \] Similarly, the more conservative procedure given by choosing the threshold $\tau_+ > 0$ where
\[ \tau_+ = \min \left\{ t > 0 : \frac{1 + \# \{ j : M_j \geq t, p_j > c \}}{\# \{ j : M_j \geq t, p_j \leq c \}} \leq q \right\} \qquad \textbf{(MultipleKnockoffs+)} \] and setting $\hat{S} = \{ j : M_j \geq t, p_j \leq c \}$ controls the usual FDR by
\[ \BE\left[ \frac{|\{ j \in \hat{S} \cap \mathcal{H}_0 \}|}{|\hat{S}| \vee 1} \right] \leq q. \]
\end{theorem}
\begin{proof}
The proof of the theorem above follows from Theorem 3 of \cite{knockoffs}. In particular, the connection with knockoffs in Section 5.2 of \cite{knockoffs} explains the connection between knockoffs and the SSTP. Here, the connection is even more explicit: we replace $|W_j|$ with $M_j$, and store the information of the $p$-values directly. In particular, from the work that we have done, it is evident that $p_j \geq \Unif[0,1]$, and we assume that they are independent from the non-null $p$-values. Finally, note that we are doing something analogous to ordering our covariates in order of decreasing $M_j$ in the procedure by looking at cutoffs for $M_j$. Hence, it is clear that the procedure outlined above is a special case of the SSTP, and we are done.
\end{proof}
A noteworthy difference between multiple knockoffs and knockoffs is the addition of the cutoff parameter $c$ for $p$-values. Indeed, since knockoffs only using extremely rough 1-bit $p$-values, a choice for $c$ in the SSTP is not particularly enlightening. In particular, selecting any $c \in (1/2, 1)$ is strictly worse than selecting $c = 1/2$, whereas selecting $c \in (0, 1/2)$ leads to zero power; it is evident that the choice $c = 1/2$ is optimal.

For multiple knockoffs, however, there is no canonical choice for $c$ in the SSTP description. For instance, consider the case where $n = 2$. Then, two candidates for $c$ exist: $1/3$ and $2/3$ are both possible. Later, we will investigate the effects of choosing different $c$ on the power of the procedure in various situations for different $n$.

\section{Constructing Multiple Knockoffs}
\subsection{A General Construction}
\todo{fill this in, not really the focus}


\subsection{The Gaussian Case}
Here, we will provide a generalization of the construction of Gaussian (or second-order) model-X knockoffs. In particular, suppose $X \sim \N(0, \Sigma)$. Then, the distribution of $(X, \tilde{X}^{(1)}, \ldots, \tilde{X}^{(m)})$ can be described by
\[ (X, \tilde{X}^{(1)}, \ldots, \tilde{X}^{(m)}) \sim \N\left( 0, \begin{bmatrix}
	\Sigma & (\Sigma - \on{diag}\{ s \})_{1 \times m} \\
	(\Sigma - \on{diag}\{ s \})_{m \times 1} & \Sigma_{m \times m} - \on{diag}\{ s \}_{\on{diag} m \times m}
\end{bmatrix}\right) \] where the notation $A_{m \times n}$ denotes the matrix $A$ being repeated in an $m \times n$ block matrix fashion, and $A_{\on{diag}{m \times m}}$ denotes the matrix $A$ being repeated in $m \times m$ diagonal block matrix fashion. From here, we can get the conditional distribution of the knockoffs $(\tilde{X}^{(1)}, \ldots, \tilde{X}^{(m)})$ in the same fashion as before: we can write
\[ (\tilde{X}^{(1)}, \ldots, \tilde{X}^{(m)}) \mid X \sim \N(\mu, V) \] where the mean and variance are given by
\begin{align*}
\mu &= (\Sigma - \on{diag}\{ s \})_{m \times 1} \Sigma\inv X = (X - \on{diag}\{s \} \Sigma\inv X)_{m \times 1}, \\
V &= (\on{diag}\{s\} - \on{diag}\{s\}\Sigma\inv \on{diag}\{s\})_{m \times m} + \on{diag}\{s\}_{\on{diag} m \times m}.
\end{align*}