
\label{chapter2}
% Since the publication of \cite{bh}, the Benjamini-Hochberg procedure has become 

Recently, Barber and Cand\`{e}s have proposed a new procedure that also controls the FDR for linear Gaussian models in \cite{knockoffs}. The procedure, known as the \emph{knockoff filter}, creates ``knockoff'' variables, which have the same covariance structure as the covariates, but are independent of the output, and uses these variables in a procedure that controls the false discovery rate. Originally, \cite{knockoffs} gave a description of the procedure in the specific case where $\b{y}$ follows a linear Gaussian model, and the covariate matrix is fixed. \cite{panning} then provided a sweeping generalization of the procedure beyond linear models. In this section, we seek to provide a self-contained exposition of the model-X knockoffs framework and procedure. In particular, we will loosely follow the structure of \cite{panning}, while borrowing necessary details from \cite{knockoffs}. After defining the problem rigorously, we will give a theoretical definition of model-X knockoffs and prove some results regarding their properties, including the procedure that controls FDR. We will then think about how to construction of knockoffs more concretely.

\section{The Model-X Paradigm}

\section{Problem Statement}
Here, we will provide a rigorous statement of the problem that we seek to solve: this is essentially identical to \cite{panning}, but for sake of thoroughness, we find it necessary to review.

Suppose we have i.i.d. samples from a population in the form $(X, Y)$. Here, $X$ is a $p$-dimensional vector $(X_1, \ldots, X_p) \in \BR^p$, and $Y$ is a scalar in $\BR$. We wish to determine if there is a smaller subset of these $X$ variables for which the conditional distribution of $Y$ depends upon: we wish to find the smallest subset $\mathcal{S}$ such that conditional on $\{ X_j \}_{j \in \mathcal{S}}$, $Y$ is independent of the other variables $\{ X_j \}_{j \not \in \mathcal{S}}$. In most use cases and situations, the set $\mathcal{S}$ will be unique. A pathological example is given in \cite{panning}, but we will not concern ourselves with these edge cases. We now provide a definition of a ``null'' covariate versus a ``relevant'' covariate, bearing in mind the context of attempting to find the desired subset $\mathcal{S}$.
\begin{defn}
A variable $X_j$ is said to be ``null'' if and only if $Y$ is independent of $X_j$ conditionally on the other variables $X_{-j} = \{ X_1, \ldots, X_p \} \setminus \{ X_j\}$. The subset of null variables is denoted by $\mathcal{H}_0$ and we call a variable $X_j$ ``non-null''or ``relevant'' if $j \not \in \mathcal{H}_0$.
\end{defn}
We may then rephrase the task at hand to looking for ``relevant'' variables while controlling the FDR. In particular, suppose we have a selection set $\hat{\mathcal{S}}$, a subset of the covariates. Then, the number of false discoveries, denoted $V$ previously, will be $\left| \hat{\mathcal{S}} \cap \mathcal{H}_0 \right|$, whereas thte total number of discoveries, denoted $R$ previously, will be $\left| \hat{\mathcal{S}} \right|$. As such, the FDP and FDR are respectively
\begin{equation}
\FDP = \frac{\left| \hat{\mathcal{S}} \cap \mathcal{H}_0 \right|}{\left| \hat{\mathcal{S}} \right| \vee 1}, \qquad \FDR = \BE \left[ \frac{\left| \hat{\mathcal{S}} \cap \mathcal{H}_0 \right|}{\left| \hat{\mathcal{S}} \right| \vee 1} \right].
\end{equation}

\section{Definitions}
\subsection{Knockoff Variables}
We will now provide the definition of model-X knockoffs, as presented in \cite{panning}.
\begin{defn}
\label{knockdef}
Model-X knockoffs for a family of random variables $X = (X_1, \ldots, X_p)$ are a new family of random variables $\tilde{X} = (\tilde{X}_1, \ldots, \tilde{X}_p)$ constructed satisfying the following two properties.
 (1) \emph{Exchangeability.} For any subset $S \subset \{ 1, \ldots, p \}$, 
\begin{equation}
(X, \tilde{X})_{\on{swap}(S)} \overset{d}{=} (X, \tilde{X}),
\end{equation}
where $(X, \tilde{X})_{\on{swap}(S)}$ is the vector obtained from $(X, \tilde{X})$ by swapping $X_i$ and $\tilde{X}_i$ for $i \in S$. (2) \emph{Conditional Independence}. $\tilde{X} \indep Y \mid X$ if there is a response variable $Y$. This property is satisfied if $\tilde{X}$ is constructed without looking at $Y$.
\end{defn}
At a high level, the knockoffs procedure will work as follows: we can generate test statistics using the original covariates as well as the knockoff covariates in the same way. Then, the original covariates that are relevant to $Y$ should generate test statistics that are siginificant relative to its knockoff, while the null covariates should not generate test statistics that are very different than its knockoffs, and this should allow us to find the relevant covariates.

The following is an important property of knockoff variables, which we will generalize later.
\begin{lem}[Exchangeability of Nulls]
\label{origexchangeofnulls}
Let $S \subset \mathcal{H}_0$ be a subset of nulls. Then
\[ [\bX, \tilde{\bX} ] \mid y \overset{d}{=} [\bX, \tilde{\bX} ]_{\on{swap}(S)} \mid y. \]
\end{lem}
\begin{proof}
Omitted, but uses \Cref{origexchangeofnulls} in the original proof. This is also a special case of \Cref{lemma2.3}, which is proved later.
\end{proof}
\subsection{Feature Statistics}
We now describe the feature statistics that we wish to use in our procedure. We wish to compute statistics $W_j$ for each $j \in \{ 1, \ldots, p \}$, where a large value of $W_j$ gives evidence that $X_j$ is a relevant (non-null) covariate. In particular, the test statistic will be a function of both the original covariates and the knockoffs, as well as a function of $y$, so
\[ W_j = w_j([\bX, \tilde{\bX}], y) \] for some function $w_j$ where larger values of $w_j$ indicates stronger evidence that $X_j$ is non-null. In addition, we also need to ensure that the function $w_j$ satisfies the \emph{flip-sign property}, which says that if we switch an original covariate with its knockoff, then the sign of the knockoff statistic is switched. Formally,
\[ w_j([\bX, \tilde{\bX}]_{\on{swap}(S)}, y) = \begin{cases}
w_j([\bX, \tilde{\bX}], y), \qquad & j \not \in S, \\
-w_j([\bX, \tilde{\bX}], y), \qquad & j \in S.
\end{cases}\]
Although the $W_j$ are a convenient way to package all of the pertinent information coming from the original covariates and its knockoffs into a single number, the statistic soon becomes unwieldy when we attempt to generalize later. As such, perhaps it is more apt to think about the $W_j$ in the following way. We can generate individual statistics that measure the importance of each covariate, call them $Z_j$ and $\tilde{Z}_j$ respectively. More formally, we can construct the vector $(Z, \tilde{Z}) = t([\bX, \tilde{\bX}], y)$ such that
\[ (Z, \tilde{Z})_{\on{swap}(S)} = t([\bX, \tilde{\bX}]_{\on{swap}(S)}, y) \] Then, the $W_j$ can be constructed by choosing an antisymmetric function $f_j$ and letting
\[ W_j = f_j(Z_j, \tilde{Z}_j). \] In particular, a choice of statistic $W_j$ is in correspondence with a choice of statistic $Z_j$ and a choice of antisymmetric function $f_j$. In our work with multiple knockoffs, we will generalize feature statistics by using $Z_j$'s as well as a symmetric function; the analog to the sign of $W_j$ will be examined separately. We restate Lemma 3.3 from \cite{panning}
 as follows.
\begin{lem}
Conditional on $(|W_1|, \ldots, |W_p|)$, the signs of the null $W_j$'s are i.i.d. coin flips. 
\end{lem}
\begin{proof}
Omitted. This is a special case of \Cref{lemma2.4}, which is proved later.
\end{proof}
The above lemma critically allows us to convert the signs of the $W_j$ statistics into one-bit $p$-values, which are then used in the proof of FDR control via viewing the knockoff procedure as a sequential selection testing procedure as in \cite{knockoffs}. 

\section{The Knockoffs Procedure}
Now that we have built the machinery of knockoff variables as well as the generation of feature statistics, we are ready to describe the knockoffs procedure.
\begin{proc}[Knockoffs/Knockoffs++]
\label{knockproc}
Suppose we are given a collection of $p$ covariates $(X_1, \ldots, X_p)$ along with a response variable $y$. Fix a desired control level $q$. Then, generate knockoffs $(\tilde{X}_1, \ldots, \tilde{X}_p)$ and compute feature statistics $W_j$ for each $X_j$ satisfying the flip-sign property defined above. Next, for the Knockoffs procedure, define $\tau$ as
\begin{equation}
\tau = \min \left\{ t > 0 : \frac{\# \{ j : W_j \leq -t \} }{\# \{ j : W_j \geq t \} } \leq q \right\}.
\end{equation}
Then, the set of covariates selected is precisely
\begin{equation}
\hat{\mathcal{S}} = \{ j : W_j \geq \tau \}.
\end{equation}
Alternatively, for the Knockoffs++ procedure, define $\tau_+$ as 
\begin{equation}
\tau_+ = \min \left\{ t > 0 : \frac{1 + \# \{ j : W_j \leq -t \} }{\# \{ j : W_j \geq t \} } \leq q \right\}.
\end{equation}
Similarly, the set of covariates selected here is 
\begin{equation}
\hat{\mathcal{S}}_+ = \{ j : W_j \geq \tau_+ \}.
\end{equation}
\end{proc}
\begin{theorem}
The Knockoffs procedure from \Cref{knockproc} controls the modified FDR, defined as
\[ \on{mFDR} = \BE \left[ \frac{ \left| \hat{S} \cap \mathcal{H}_0 \right|}{|\hat{S}| + 1 / q} \right] \leq q. \] The Knockoffs++ procedure from \Cref{knockproc} controls the usual FDR.
\end{theorem}
\begin{proof}
The proof comes from Theorem 1 and 2 of \cite{knockoffs}, which are respectively proven via Theorem 3 of \cite{knockoffs} by framing the knockoffs procedure as a sequential testing procedure. We will not press into the details here, as they will be explored in more detail later on.
\end{proof}

\section{Building and Using Knockoffs}
Now that we have described the knockoffs procedure, the question of implementation still remains. In particular, how might one go about constructing the knockoff variables and sampling them? In addition, what are the benefits to using this procedure as opposed to simply applying Benjamini-Hochberg: is it clear that there are significant benefits to using this method? In this section, we will first provide an example of how to construct knockoff variables in the Gaussian case and briefly discuss a more general algorithm for sampling knockoff variables.   
\subsection{The Gaussian Case}
Consider the case where $X \sim \N(0, \Sigma)$. Then, the distribution of $(X, \tilde{X})$ can be
\[ (X, \tilde{X}) \sim \N \left(0, \begin{bmatrix}
	\Sigma & \Sigma - \on{diag}\{s \} \\
	\Sigma - \on{diag}\{s \} & \Sigma
\end{bmatrix} \right). \] However, in many cases, we will be interested in the conditional distribution of $\tilde{X} \mid X$: if we have already observed $X$, then what is the conditional distribution of the knockoffs $\tilde{X}$ In the case of Gaussian knockoffs, this is readily answered by the classical formula
\[ \tilde{X} \mid X \sim N(\mu, V) \] where the mean and variance are given by
\begin{align*}
\mu &= X - X \Sigma\inv \on{diag}\{ s \}, \\
V &= 2 \on{diag}\{ s \} - \on{diag}\{ s \} \Sigma\inv \on{diag} \{ s \}.
\end{align*}
In practice, this allows us to sample knockoff variables quite easily when we have observed values of $X$ from a Gaussian distribution. Note that above, we are able to choose the values of $\on{diag}\{s\}$ for ourselves, under the constraint that the covariance matrix of $(X, \tilde{X})$ is positive semi-definite. In particular, we will want to ``maximize'' the values of the diagonal matrix $\on{diag}\{s\}$ in order to maximize the power of the knockoff procedure, as we will want to construct knockoffs that are not too similar to the original covariates. There are a few different ways that we may seek to do this as follows.

\subsubsection{Equicorrelated Knockoffs}

\subsubsection{Other}

\subsection{A General Construction of Knockoff Variables}
In general, the problem of sampling knockoffs can be quite hard and remains a hard question posed. \cite{panning} offers an algorithm called the Sequential Conditional Independent Pairs algorithm to deal with the general case, but for the purpose of this paper we will not devote much time to thinking about the general case. Although it is not too difficult to generalize the Sequential Conditional Independent Pairs to multiple knockoffs, we will mostly focus on generalizing the case of Gaussian covariates in order to obtain more information about the power and effectiveness of using multiple knockoffs as opposed to other methods. Note that \cite{panning} provided significant evidence that using the model-X knockoffs procedure was quite effective and led to significant power gain in certain scenarios, and we hope to do the same (or disprove the effectiveness of multiple knockoffs) in later parts of this paper.